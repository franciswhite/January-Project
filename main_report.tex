\documentclass[10]{article}
\usepackage[utf8]{inputenc}

\title{January Project}
\author{Silvan Hungerbuehler, Haukur P. Jónsson}
\date{Date quo: 1.02.17}

\usepackage{amssymb}
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{comment}
\bibliography{references.bib}
\begin{document}
\maketitle

\section{Introduction}
When Donald Trump, the current president of the United States of America, was asked to explain calling women ''fat pigs'', ''dogs'', ''slobs'' and ''disgusting animals'' his reply was:
\textit{I’ve been challenged by so many people, I don’t frankly have time for total political correctness. And to be honest with you, this country doesn’t have time either.'}
%https://www.theguardian.com/us-news/2016/nov/30/political-correctness-how-the-right-invented-phantom-enemy-donald-trump
In contrast to Mr Trump, the authors of the present paper had time. We provide a formal framework in order to analyze the pragmatic phenomenon of harmful use of language.

We are interested in explaining under which circumstances speakers will be disposed to alter their way of expressing themselves. Specifically, we will analyse the situation where a speaker is indifferent about various communicative options, while the audience dislikes some of the alternatives. Although the wording of a message might not affect the transfer of information, it can have an impact on the listener. Obvious cases in point are racial slurs and so-called ''trigger words'' which cause listeners suffering from postraumatic stress disorder (PTSD) to re-live past trauma. \cite{fagan2004confronting, yehuda2002post} So although two expressions might be semantically identical, choosing one over the other can have a substantial effect on actual communication, depending on who is speaking, and, crucially, who is listening. We thus take the pragmatic slogan seriously that there is far more to communication than the literal meaning of the expressions employed.
When studying the use of language as the transfer of information, the issue becomes to explain how there can coexist equally good - expressive, accurate, etc. - forms of communication, yet some options are used rather than others.

Based on our model we will present three core results: First, we will offer a formal explanation for the reason why offensive use of language is so persistent. Given that there are many ways of putting things, how come people still offend and harm each other?  Second, we will argue that there is potential for better communication, benefitting everyone. Third, we will argue that the degree of ''empathy'' language users exhibit is of crucial importance to reap those benefits.

We will rely on game theoretic models of the \textit{Iterated Best Response} (IBR) family. This approach looks to explain and predict pragmatic inferences of natural language users by modeling a forth-and-back reasoning taking place between two agents engaged in conversation.
Our main formal addition to such models is that we want to forgo the common assumption that it is exclusively the player sending a message for whom this is is costly. We will look at signaling games where costs are conncected not only to the sender, but also to the recipient.
We will offer some take on these questions by following this outline:
\begin{enumerate} %need to make this prose. but will do so later, once order is established.
\item Brief Introduction to Signaling Games
\item Introduction to the IBR Model
\item Sticks and Stones
\item Conclusion, Unexplored Topics \& Further Research
\end{enumerate}

\section{Game Theory and Pragmatics}
\subsection{Signaling Games}
Signaling games are sequential games between two players: a sender and a receiver. In the context of linguistic modeling one can usually think of the sender as a speaker and of the receiver as a listener. The game is playerd in some particular state of the world, formally an element of set $T$ which represents possibles ways the world could be. Only the sender knows which state this happens to be. The receiver, however, has some beliefs about the possible states. These beliefs are modeled with a probability measure, so that $T$ is sample space and $P_R$ is probability measure which is a function $P_R: T\rightarrow \mathbb{R}_{\geq 0}$ satisfying $\sum_{t\in T}P_R(t)=1$.

The game is then played such: After the sender observes the true state of the world $t$ she sends some message $m_i$ out of a set of messages $M$.
The receiver then observes which element $m_i$ from $M$ was sent and chooses some action $a_j$ from a set of actions $A$.
The players' respective utility is given by functions $u_S$ and $u_R$. They take as arguments the state the players find themselves in and the action performed by the receiver, so that $u_S(t,a)$ and $u_R(t,a)$. We will only consider cooperative games, so that $u_S(t,a)=u_R(t,a)$ for $\forall t\in T$ and $\forall a\in A$.
What message is being sent does not directly affect the players' payoff, for the message is not part of the utility function. Hence, the only effect a message can have is by altering the receiver's belief. Different beliefs have the potential to alter her optimal action and thereby to affect both players' payoff.
The sender's strategy is a function from the set of states $T$ to the set of messages $M$; the receiver's strategy is a function from the messages $M$ to the set of actions $A$.
%We could already introduce priors, cost and expected utility here.
%A pure strategy is thus a complete plan of action which tells the players exactly how to react in any situation in the game where they have the power to decide what action to take.
%\textit{Pure} here simply means that a player will choose a some action with certainty, that is, always, when confronted with a given situation. In contrast to pure strategies stand
%\textit{mixed} strategies where players will choose from various actions, each with a certain probability.


\subsection{Extension towards pragmatics}
Game theoretic modeling has been used for of a variety of phenomena related to information transmission. %EXAMPLES: biology paper, job market signaling,
Amongst other disciplines, it was applied in linguistics, specifically in attempts to obtain a firmer conceptual grip on Gricean pragmatics. Here the communication between (rational) agents is thought of as a game played between interlocutors. Formulating situations where pragmatic phenomena arise with mathematical precision helps in different ways: On the one hand, it allows bridging the gap between theoretical work on pragmatics and actual experimental data with language users. More specifically, it generates testable predictions about language use under a variety of parametrizations, as pertaining to the agent's (limited) rationality, cognitive ability to think multiple steps ahead, their preferences and methods of deciding on what action to take - so-called \textit{choice rules} %References? perhaps paolo's phd thesis?
 - or their beliefs about the world. On the other hand, such models ought to provide insight into how agents actually arrive at the linguistic behavior they exhibit. %Do you like the last sentence? or do you prefer "...arrive at their linguistic behaviour"?
 Indeed, models concerned with pragmatic behavior of linguistic agents are caught between the pressure of trying to accurately predict and fit experimental data and the hope to realistically capture one or the other aspect of a pragmatic phenomenon - to represent ''what actually is going on'', so to speak.

The family of models we will be concerned with tries to capture the agents' mutual reasoning about each other. Which action the players deem optimal for them and perform depends on a process of reasoning about what the respective other believes. The model tries to capture this process of reasoning about what the respective other does, believes that her opponent will do, believes what her opponent will believe that she will do, and so forth. %Our model will be an extension of this ''Pragmatic Back-and-Forth Reasoning'' \cite{franke2014pragmatic}.
%Should we mention it explicitly? or just name it as a reference when we give the introduction to the workings of the model?
\subsection{The IBR model}
We are going to use the \textit{iterated best response} (IBR) to provide our solution concept. %WHY?
For details and discussion see \cite{franke2009signal} and \cite{franke2014pragmatic}. IBR relies on an explicit representation of the agents' beliefs about each other. The interlocutors beliefs are spelled out in a round-for-round manner in order to capture what intuitively could be the described as the epistemic dynamic of: ''I think that you think that I think that you think...etc.''. Language users are represented to have some hierarchically ordered level of sophistication which correspond to the number of steps an agent can reason forth and back between herself and the other agents. The reasoning chain formed by such steps is then a sequence of iterated optimal responses. It is bounded by the maximal depth a player of a given level of sophistication - or strategic type - can go to.

Entirely unsophisticated types, players who do not take the opponents reasoning into account at all and are thus completely unstrategic, are assigned the level-0. In the games under discussion in this paper this will mean that senders of level-0 will be only concerned with saying whatever is true according to the semantics of the language fragment in question, while receivers of that level will always interpret a message they receive literally. An agent of level-$k$ then forms some belief about the other's behavior who, by assumption, is of some level $l$, where $k\geq l$. Intuitively, this can be interpreted as the player looking down the sophistication hierarchy in order to build some rational expectation of what the other of some specified level $l\leq k$ will do. It is common to assume for simplicity that players of level $k$ expect their opponents to reason exactly one step less than themselves and are thus of level $k-1$.

As in standard signaling games, $T$ is the set of states the players can be in, $M$ the set of messages at the sender's disposal and $A$ the set of actions from which the receiver can choose. Assume for now that $T,M$ and $A$ are all countably finite. A sender strategy is a row-stochastic $(|T| \times |M|)$-matrix $\sigma$, and a receiver strategy is a row-stochastic $(|M|\times |A|)$-matrix $\rho$. By way of example, here are two strategies for a game with $|S|=|A|=3$ and $|M|=2$, so three states and actions, and two messages. \\
%Do we need the examples? Do we have space for the examples?
%This whole section can be made shorter.
\begin{equation*}
\sigma =
    \bordermatrix{
          & m_1 & m_2    \cr
      t_1 & 0.2 & 0.8  \cr
      t_2 & 0.4 & 0.6  \cr
      t_3 & 1 & 0
    }\qquad
\rho =
    \bordermatrix{
              & a_1 & a_2 & a_3    \cr
          m_1 & 0.5 & 0.5 & 0  \cr
          m_2 & 0.8 & 0.1 & 0.1
        }\qquad
\end{equation*}

When the sender observes some state, her strategy determines with which probability she chooses from the different messages; likewise for the receiver, but with messages and actions. The rows in the strategy matrices thus represent the situations where the players get to decide, and the respective entries in the row describe the probabilities with which they opt for the move in that column. The sender, in this example, would choose $m_1$ with probability $0.2$ upon observing $t_1$, and with probability $1$ when observing $t_3$. The receiver, in turn, would choose $a_1$ and $a_2$ each with probability $0.5$, while never playing $a_3$, when receiving $m_1$ from the sender.
%Here is the place to use the definition of pure and mixed strategies. If we actually want to define them before.

The meaning of the language fragment in question is modeled as a $(|T| \times |M|$-matrix. If some state $t_i$ semantically warrants the utterance of some message $m_j$, then the entry $x_{ij}$ in the matrix will be $1$, otherwise $0$. This matrix reporting the boolean values is called the \textit{Boolean Matrix} and denoted by $B$. For the purpose of illustration, an example where both $t_1$ and $t_2$ can be distinctly expressed, but the game contains no message meaning $t_3$:\\
%Do we need this example? I would say it's the first to go if we run short of space.
\begin{equation*}
B =
    \bordermatrix{
              & m_1 & m_2    \cr
          t_1 & 1 & 0  \cr
          t_2 & 0 & 1  \cr
          t_3 & 0 & 0
        }\qquad
\end{equation*}

The strategies of level-0 type are represented by the normalized boolean matrix. Normalization here simply means a mapping of some $(m\times n)$-matrix $A$ onto some other $(m\times n)$-matrix $B$ such that $B_i\propto A_i$ if $\sum_j (A_{ij})>0$ and $B_{ij}=\tfrac{1}{n}$ otherwise. In the case of the receiver the transposed boolean matrix is normalized.

More sophisticated players are modeled as to have some belief about the interlocutors behavior. For now we will assume that players have unbiased beliefs, meaning that there are no further restriction imposed - for example, that some player simply disregards a possible strategy. A belief is unbiased if all elements $x\in X$ of some (finite) set X are deemed to be equally probable. All $y\not\in X$ are assigned probability zero. So that each option is considered equally likely. We thus follow Franke's argument for
%Franke Phd also contains probabilistic argument about the Principle of Indifference
 using unbiased beliefs about possible opponent behavior as it allows us to simplify the mathematics and allows for more straightforward computation in linguistic applications. Formally, if $X$ is some ordered set of strategies of cardinality $\mathbb{C}$, then the set of beliefs $\Pi$ is defined as\\
\begin{equation*}
\Pi(X)=\{\sum_{x\in X} \dfrac{1}{\mathbb{C}}X\}
\end{equation*} %i don't understand this equation

%HERE WE NEED A PARAGRAPH ABOUT PRIORS FOR RECEIVER AND COST VECTORS IN GENERAL. this is also the chance to mention the vector notation later applied.

Finally, we define the notion of \textit{best response} given a set of beliefs. A sender's best response to some strategy $\rho$ is any pure strategy that maps each state to that signal which maximizes the expected utility.
More formally speaking, the sender's best response to some receiver strategy is\\
\begin{equation*}
BR_S(\rho)=\{s\in S | s_{ij}=1\implies j \in arg_kmax(U_S T(\rho)-c)_ik)\}
\end{equation*} 
A receiver's best response to a sender's strategy $\sigma$ is defined essentially the same way. Except that there may be so-called \textit{surprise messages} $m_j$ for which $\sigma_{ij}=0$, for all $i$. These are messages the sender will use with probability 0, that is, never. Since strategies are plans of action for every conceivable contingency that might arise in the course of a game, we'll assume that a receiver will fall back on her prior beliefs about the true state and play her best response.
%I believe that there is an inconsistency between the way we define BR and the way we applied it. This is not good.
%One way out: Define BR to be possibly mixed. If there is no strict best response, then randomize between the two that are equal.
As the sender has a set of beliefs $\Pi$ about the receivers possible behavior, the set of best responses is the union of all best responses to these possible strategies: $BR(\Pi=\Cup\{BR(\pi)|\pi\in\Pi\}$.

\section{Sticks and Stones}
\subsection{Exposition of Phenomenon/Interpretation}
%This needs to be introduced.

To make this more clear, consider the following scenario: Two people sharing some language engage in conversation, one as the speaker, the other as the listener. The speaker would like to communicate to the listener what kind of work a mutual acquaintance of the two does. The listener does not yet know what her occupation happens to be, but would like to do so. The acquaintance could either be an ecologist or a geologist. To transfer this knowledge the speaker can utter either ''She's a geologist.'', ''She's an ecologist.'' or ''She's a professional treehugger, stupdily trying to protect plants and stuff.''. Assume that uttering the first is true just in case the person is a geologist and the other two if she's an ecologist. As both of the two well know, the listener cares dearly for the environment, while the speaker does not. The listener, although she understands perfectly well what the speaker is trying to convey to her, would be deeply offended by her uttering the word ''treehugger''. The speaker herself couldn't care less what words she uses, as long as the message comes across and the hearer adopts the right belief about the mutual acquaintance's profession.

This is an interesting situation, for the the speaker could perfectly well achieve her goal of communication and not use the word ''treehugger''. The listener, who's not in any position to choose the signal, then has to accept the speaker's decision grudgingly, knowing that there would have been a better way for her. Alas, it was not the better way for the sender and was thus not chosen. This situation can be modeled fittingly using the IBR approach introduced in previous sections. We will do so, and then carry out further analysis on the baseline set by this model.
%
\subsection{Asymmetric Model}
\subsubsection{Game Structure}
Let us now use the IBR model to model the scenario above. Assume that

For completeness sake we expand scenario by adding one more utterance ''She's a professional stonehugger who cares more about lifless things than people''

 The  communication where triggering can occur. We define two players which are talking with each other using some shared language fragment. Although they have this common language, they are triggered by different messages. Say that a geologist and an ecologist are talking and they discuss each others professions. Thus we define the states as $T=\{geologist, ecologist\}$ and set $T=A$. They both share the goal of successful communication, thus we set their utilities such that when the speaker wants to communicate $t_{j}$ by some message $m_i$ then they both have equal positive utility when the receiver to associates $m_i$ with $t_{j}$. Thus we have:

\begin{equation*}
\bordermatrix{
      & t_{geologist} & t_{ecologist}    \cr
  t_{geologist} & a,a & 0,0  \cr
  t_{ecologist} & 0,0 & a,a
}\qquad
\end{equation*}
Where $a>0$.

Let us now assume that we have four messages $M=\{geologist, stonehugger, ecologist, treehugger\}$ with the given sematic meaning:
 \begin{equation*}
 B =
 \bordermatrix{
            & m_{geo} & m_{stone} & m_{eco} & m_{tree}    \cr
   t_{geo}  &       1 &         1 & 0       & 0 \cr
   t_{eco}  &       0 &         0 & 1       & 1
 }\qquad
 \end{equation*}

 And further assume that $m_{stonehugger}$ is a trigger message for the geologist and $m_{treehugger}$ is a trigger messages for the ecologist. We therefore define $c^G$ as a cost vector for the geologist and $c^E$ for the ecologist:

  \begin{equation*}
  c^G =
  \bordermatrix{
             & m_{geo} & m_{stone} & m_{geo} & m_{tree}    \cr
             &       0 &         c & 0       & 0
  }\qquad
  c^E =
    \bordermatrix{
               & m_{geo} & m_{stone} & m_{geo} & m_{tree}    \cr
               &       0 &         0 & 0       & c
    }\qquad
  \end{equation*}

Where $c>0$.

Furthermore we assume that the receiver is unbiased with respect to which state holds, thus $p=(0.5,0.5)$.

Usually language users will encounter the expressions of their language many times over the course of their communicative lives - both as listeners as well as speakers. To capture this we'll posit a third player - nature - which decides at the beginning of the game who of the two players gets to talk and who gets to listen. As rational language users they will need to have some plan of action for all roles, states of the world or messages they encounter. During game play we assume that the geologist is the speaker and the ecologist the receiver thus $c^R=c^E$ and $c^S=c^G$.

\subsubsection{Game Play}
As a first step, let's determine what the naive players' strategies are:\\
\textbf{Level-0 Players}\\
\begin{equation*}
S_0=Norm(B)=
\bordermatrix{
            & & & &    \cr
 &       0.5 &         0.5 & 0       & 0 \cr
 &       0 &         0 & 0.5       & 0.5
 }\qquad
R_0=Norm(T(B))
\bordermatrix{
            &  & \cr
    & 1 & 0 \cr
     & 1 & 0 \cr
     & 0 & 1 \cr
     & 0 & 1 \cr
 }\qquad
\end{equation*}
The naive sender has no principle to decide between sending either $m_{geo}$ or $m_{stone}$ when whishing to communicate $t_{geo}$. Similarly she has no principle to decide between sending either $m_{eco}$ or $m_{tree}$ when whishing to communicate $t_{eco}$. This is represented by her sending each message half of the time. So she plays a mixed strategy. The receiver's actions are clearly determined by a pure strategy.\\
What is the best response of the level-1 hearer and speaker? Following the IBR procedure we transpose $R_0$, subtract the cost vector $c^S$ and check row-wise for the best response to obtain $S_1$. We proceed analogously for the receiver, the main difference being in this case that the receiver faces some actual cost, $c^R$. This then yields strategies for level-1 players:\\ %TODO : we need to add the prior. %
\textbf{Level-1 Players}\\
\begin{equation*}
% These matrices look strange when rendered
S_1= BR_S(T(R_0)-c^S)=
BR_S(
\bordermatrix{
            & & & &    \cr
 &       1 &         1 & 0       & 0 \cr
 &       0 &         0 & 1      & 1
 }\qquad
-
\bordermatrix{
            & & & &    \cr
 &       0 &         c & 0       & 0 \cr
 }\qquad
)
\end{equation*}
\begin{equation*}
=BR_S(
\bordermatrix{
                & & & &    \cr
     &       1 &         1-c & 0       & 0 \cr
     &       0 &         0 & 0.5      & 0.5
 }\qquad
 )
=
\bordermatrix{
                 & & & &    \cr
      &       1 &         0 & 0       & 0 \cr
      &       0 &         0 & 0.5      & 0.5
  }\qquad
\end{equation*}
\begin{equation*}
R_1=BR_R(T(S_0)-T(c^R)=
BR_R(
\bordermatrix{
            &  & \cr
    & 0.5 & 0 \cr
     & 0.5 & 0 \cr
     & 0 & 0.5 \cr
     & 0 & 0.5 \cr
 }\qquad
-
\bordermatrix{
  & \cr
    & 0 \cr
     & 0 \cr
     & 0 \cr
     & c \cr
 }\qquad
)
\end{equation*}
\begin{equation*}
=
BR_R(
\bordermatrix{
            &  & \cr
    & 0.5 & 0 \cr
     & 0.5 & 0 \cr
     & 0 & 0.5 \cr
     & 0-c & 0.5-c \cr
 }\qquad)
=
\bordermatrix{
            &  & \cr
    & 1 & 0 \cr
     & 1 & 0 \cr
     & 0 & 1 \cr
     & 0 & 1 \cr
 }\qquad
\end{equation*}
We see that $R_1=R_0$ thus we conclude that $S_2=S_1$.
\begin{equation*}
R_2=BR_R(T(S_1)-T(c^R)=
BR_R(
\bordermatrix{
            &  & \cr
    & 1 & 0 \cr
     & 0 & 0 \cr
     & 0 & 0.5 \cr
     & 0 & 0.5 \cr
 }\qquad
-
\bordermatrix{
  & \cr
    & 0 \cr
     & 0 \cr
     & 0 \cr
     & c \cr
 }\qquad
)
=
\bordermatrix{
            &  & \cr
    & 1 & 0 \cr
     & 0.5 & 0.5 \cr
     & 0 & 1 \cr
     & 0 & 1 \cr
 }\qquad
\end{equation*}
% These section below requires some work. I tried to collect together all relevant parts of our arguments and threw the rest away. You can check the commit if you miss something. "Player 1 and player 2 have not been introduced at this point.
$S_3=S_2$ and $R_3=R_2$. So we have reached a stable set of strategies. Some remarks about these strategies are in order. First, the sender will never send the message she finds offensive. This makes perfect intuitive sense, because she has a non-costly alternative to spare her from uttering "autist". Second, the receiver - Player 1 - view the message $autist$ as a \textit{surprise message}. That is, although Player 2 will never play it, Player 1 still needs to specify what would happen in this contingency. We'll assume that she simply uses her (unbiased) priors to decide. Thirdly, while a sender of level zero still harms herself by blindly sending a message she dislikes, every sender of level above zero will stop doing so. Thus, the costly message for sender will not be seen on higher levels and her expected payoff always be optimal.

The following table summarizes the third point:
\begin{table}[h]
\centering
\caption{Expected Payoff for Level of Sophistication}
\label{my-label}
\begin{tabular}{lll}
                                    & Sender                                  & Receiver                                \\ \cline{2-3}
\multicolumn{1}{l|}{Level-0}        & \multicolumn{1}{l|}{$a-\tfrac{o^S}{4}$} & \multicolumn{1}{l|}{$a-\tfrac{o^R}{4}$} \\ \cline{2-3}
\multicolumn{1}{l|}{Level-k, $k>0$} & \multicolumn{1}{l|}{a}                  & \multicolumn{1}{l|}{$a-\tfrac{o^R}{4}$} \\ \cline{2-3}
\end{tabular}
\end{table}
\subsection{Informal problem characterization}
Successful communication will yield a payoff of $a$ for both players. When played as an IBR, there are essentially two possible games: one where Player 1 is in the role of the sender and Player 2 in the role of receiver, and vice versa. Since the game is entirely symmetrical it will suffice to analyze just one such situation. The players' reasoning, and ultimately behavior, will be the same as their opponent's were they to find themselves in their position.
The interesting story here, however, are their expected payoffs. While playing this strategy yields an expected payoff of $a$ for the sender, the receiver obtains a mere $a-\tfrac{c}{4}$ in expectation.
\footnote{The calculation leading to this: With probability 0.5 state $t{eco}$ holds where the receiver sends $m_{tree}$ with probability 0.5. So $m_{tree}$ is sent in $0.5\times 0.5 =0.25$ of all cases and then causes a cost of $c$. Communication is always successfull, as can be gathered from the strategies played, hence the receiver can expect to obtain the payoff of correct interpretation minus the cost$\times$probability of hearing $m_{tree}$.}To see why this is, consider the positions the two players are facing in this game. Start with the receiver: Whether she likes the message or not, she can always reason what her interlocutor wants to communicate to her. If the messages $m_{geo}$ or $m_{eco}$ are sent, she will chose the respective action, communication is successfull, so both players get their payoff and everyone is happy. If the message $m_{tree}$ is sent, however, she immediately suffers the associated cost. Still, she knows what the sender is trying to communicate and her best response will be to comply and play the respective action. Refusing to understand is not an option, for it would only add insult to injury; she would both pay the cost of hearing $m_{tree}$ and lose the payoff of understanding correctly. Because she can anticipate the receiver to behave this way, the sender does not have to care about sending either semantically correct message upon observing that the state $mathematician$ holds.

Since we assumed that either state holds with equal probability, and the sender uses $m_{tree}$ in half of the cases when signaling $ecologist$, we can expect the receiver to pay the cost of $c$ every fourth time around. This results in $EU(R)=a$ and $EU(S)=a-\tfrac{c}{4}$.

Now, actual language use is an activity which agents play repeatedly and in different roles, so let's assume they play above game many times while nature choses the players' roles in each round with equal probability. Since, as already noted, the game is symmetrical, we can simply interchange the expected payoffs for the two players if they play in the other role. Say, nature happens to be very fair and assign the role of speaker with probability 0.5 to either player. Over the course of a lifetime of conversation - or a sufficient amount of linguistic interactions (thanks Gawd for the weak law of large numbers, dicho sea de paso) - the players can expect a utility of $a-\tfrac{o^i}{8}$, for $i\in \{1,2\}$. Notice that $a-\tfrac{o^i}{8} > a-\tfrac{o^i}{4}$, for any $o^i,a>0$. %is this right?

Translating this to our signaling game model would imply that there must be some incentive for players finding themselves in the role of the sender to behave differently than our model's predictions. We will presently speak of possible ways to capture these forces, but let us first look closer at the way in which optimality fails.

At a simple gaze, there simply are better solutions in the game than the ones found so far. Better in which way? Better in the sense, that in both games there exists a solution which provides a greater expected utility for the receiver without affecting the senders expected utility. Simply put, if the sender were to refrain from sending messages which are costly for the receiver, but is still able to convey the correct meaning, then everyone is better or at least as well off. In terms of finding an intuitive name for the interlocutor's behavior, the notion of \textit{political correctness} seems to capture it quite well. In terms of providing a technical explanation for why equilibria thus reached are better, the notion of \textit{Pareto optimality} can be employed.


\subsection{Pareto Aparatus}
The previous section contained a demonstration of how receiver costly messages can be modelled. Furthermore, we showed how the IBR process yields strategies which are sub-optimal for both players. In fact, they happen to be optimal for the sender who is in charge and at liberty to send harmful messages to the receiver. We will momentarily bring to the for what criterium of optimality fails to be satisfied. Meanwhile, the receiver's interests are not directly relevant for sender the sender and therefore easily discarded. This seems at odds with real-life observations of language use. It would seem that speakers sometimes do consider the listeners' interests into account when deciding how to express themselves. As a matter of fact, it seems to have become quite unfashionable in many social contexts to call people by their respective racial slurs over the last couple of decades. There seems to exist some understanding between language users which goes counter to the predictions our IBR model gave so far.

Formally, let $S$ be a set of strategies, let $N$ be a set of players, from 1 to n, and let $u_i$ be the corresponding utilty functions for each player. We say that a strategy $s \in S$ Pareto dominates another strategy $s' \in S$ if:

\begin{equation*}
\forall i \in N: u_i(s) \geq u_i(s') \land \exists j \in N: u_j(s) > u_j(s')
\end{equation*}

Then we define a strategy s to be Pareto optimal there is no other strategy profile s' which Pareto dominates it. %not sure if formula is 100% correct.

%\begin{equation*}
%\forall i \in N: u_i(s) \geq u_i(s') \land \neg \exists j \in N: u_j(s') > u_j(s) % i think there needs to be some quantification over strategies
%\end{equation*}

When looking at stable strategies of a given game, one can compare the players' utilities when using each strategy. If some alternative strategy gives any player a higher utility while no other player's utility decreases, we say that the alternative \textit{pareto dominates} the first option. Playing the alternative means that the average utility is increased. However, playing the alternative strategy is not always viable. if the players fail to coordinate sufficiently, one or both of them might end up worse.
\subsection{Formal problem characterization}
We claim that the receiver could be just as well off as the sender without the latter having to lose anything in return. The strategies played fail to satisfy a criterium of optimality known as \textit{pareto optimality}. Let us now compare the IBR result to another strategy:

\begin{equation*}
S_{BR}=
\bordermatrix{
            & & & &    \cr
 &       1 &         0 & 0       & 0 \cr
 &       0 &         0 & 0.5      & 0.5
 }\qquad
R_{BR}=
\bordermatrix{
            &  & \cr
    & 1 & 0 \cr
     & 0.5 & 0.5 \cr
     & 0 & 1 \cr
     & 0 & 1 \cr
 }\qquad
\end{equation*}
Yields expected utility ...
... balblablalb
Need to finish this part showing the Pareto domination of the pruposed strategy

\section{Solution concept}
We therefore want to adjust the IBR model so that it is able to reach this Pareto optimal solution. This can be done by adjusting the sender's stratey such that she refrains from using the message associated with receiver's cost. That is, the sender never sends the offending message and that message is considered by the receiver as a surprisal message.

We can clearly see that there is a strategy, in both games, which is better than the strategy reached by the IBR model, namely, the strategy above. This optimal strategy could be reached by only adjusting the sender strategy so that she never sends the offending message. We don't see this strategy in the IBR model since it is not explicitly in the sender's interest and, coincidentally, it is her who wields the power to decide over the matter. Thus we might adjust the sender preferences so that she takes into account the receiver's interests, i.e. the receivers cost. In the next section we discuss some ways to achieve this.\\

\subsection{Solution statement informal}
If both players refrain from using harmful/costly messages and instead use other messages which convey the same meaning then a Pareto optimal solution is possible. This could be achvieved by having a pre-game talk in which the two players could agree on using only strategies which both agree wwith. This would allow them to move to the pareto optimal equilibrium of strategies. This does not seem so far fetched since it seems in language user's best interests to opt of Pareto optimal solutions, since someday the tables might turn.

How do we then go about adjusting the model in order to take the receiver's preference explicitly into account? One solution might be to define three types of sender types: \\

\begin{enumerate}
\item A nice sender
\item A malicous sender
\item Indifferent sender
\end{enumerate}

\subsection{Solution statement formal}

Then for the \textit{Nice sender} we extend the sender's cost vector:

If we generalize the \textit{nice} and \textit{malicious} definitions of the sender's cost vector we get a new adjusted sender cost vector.

\begin{equation*}
c_s^a= c_s + a * c_r
\end{equation*}

Where a is a scalar value, s.t. $a \in \mathbb{R} $. So if $a=0$ we get an indifferent sender, if$a <0$ we get a malicious sender and if $a>0$ we get a considerate sender.

Show simply how a playthrough works

\subsection{Conclusion}

\subsection{References}

\section{Appendix}
\subsection{Threats \& Punishments}
The receiver can try different strategies in order to encourage the sender to use Pareto optimal strategies. The Pareto optimal strategies we have presented thus far only improve the receiver's utility not the sender's. So one might ask; Can the receiver force another stragey which would be sub-optimal for both. Thus making a strategy change in the sender's explicit interest? If so, would this enforcement be credible? A credible enforcement is a strategy which does not give a worse outcome to receiver but gives a worse outcome to the sender. Let us now consider that the receiver issues threats to the sender in a pre-game talk. %fix ending%
%do we formalize this? Damn this seems terrible%
\subsubsection{Threatning miscommunication}
Let us now consider that the receiver issues a threat to associate costly messages with another action. The receiver can then choose another action to associate with that message by some distribution. (We assume that the receiver has to associate the message with something, otherwise the whole game breaks down.) Let us now consider a few distributions the receiver can use:

A uniform distributions over all actions. \\
A uniform distributions over all actions excluding actions which yield any positive utility/ where the cost is high. \\%(Here it would be useful to define cost as a function of message and state/action. Thus we could get rid of the cost by associating it with another action)
Over the priors of states. \\

Using the uniform distribution the receiver will sometimes remain in status quo by associating the message with an action which yields positive utility for the sender, that is, the sender has a utility of $\mu^S=a$ and receiver $\mu^R=a-c$. When associating the message with another action which does not yield any positive utility to the sender the sender has a utility of $\mu_S=0$ and receiver $\mu_R=0-c$. %(Here it would be useful to define cost as a function of message and state/action. Thus we could get rid of the cost by associating it with another state)%
This gives:
$$EU(S)=a-a \cdot P_C \cdot P_S$$
 where $P_C$ is the probability of using a costly message and $P_S$ is the probability of associating the message with an action which does not yield a positive utility.
 $$EU(R)=a-a \cdot P_C \cdot P_S - c \cdot P_C$$
 Using the asymmetrical example above we get
 $$EU(S)=a-a \cdot 1/4 \cdot 1/2=7/8 \cdot a$$
 $$EU(R)=a-a \cdot 1/4 \cdot 1/2 - c \cdot 1/4=7/8 \cdot a - 1/4c$$
 The same result as before but both receiver and sender with $a \cdot P_C \cdot P_S$ lower utility as before. Therefore it is not credible that the sender will use this distribution.

A uniform distributions over all actions excluding actions which yield any positive utility we essentially set $P_S=1$. But this does not change the final outcome. (In fact it can be shown with an IBR model that in the asymmetrical example by associating the costly message with the other action the sender will use the costly message to associate that action. Thus with a small message space this can prove destructive strategy for the receiver)

Similarly the sender can use the prior distribution to her advantage.

We therfore conclude that the threat of associating costly messages with other actions is not credible.

\subsubsection{Mixing messages}
Let us now consider the threat that when receiver receives a costly message she will interpret it as another message. %how do we argue this? this sounds terrible...%
That is when receiving message $m_i$ she will act as if she received message $m_j$. By doing this we say that she avoids the costly messages all together. Again it becomes a problem of how she decides which message to confuse it with, i.e. a problem of which distribution to choose. Furthermore when choosing another message she has to consider the utility of the receiver and her self of that message. Since if she chooses another message which has a positive utility for the sender and is costly for her she is back to square one. We therefore only consider messages which give no utility for the sender and are not costly to the receiver.

Let us now go through the case when the sender sends a costly message and the receiver interprets it as some other message and compare to our original example. The sender sends a costly message and the receiver interprets it as some other message and associate the state with an incorrect action, thus $\mu^S=0$ and $\mu^S=0$ since successful communication was not reached. Compared to the normal strategy of accepting the costly message and achieve a successful communication with $\mu^S=a$ and $\mu^S=a-c$. %Here there are some missing steps (doing the EU) which we can add.
We can therefore see that $EU(R)$ increases but $EU(S)$ decreases. We can therefore see that if $a-c \leq 0$ then mixing the messages is a credible threat for the receiver.

\end{document}
